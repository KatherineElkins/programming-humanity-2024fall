{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# JAX as example of functional programming for speed\n",
        "\n",
        "Modified by K. Elkins  2024\n"
      ],
      "metadata": {
        "id": "5Qdo6__7IOsi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configure Notebook"
      ],
      "metadata": {
        "id": "DyCW0fmPeWhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n"
      ],
      "metadata": {
        "id": "hhF7rdkceWU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "ou9o3ONw34mt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "nXnrITSf34gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from random import random, shuffle"
      ],
      "metadata": {
        "id": "y-ifd9bhZhDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools"
      ],
      "metadata": {
        "id": "lz9Ed22Abnd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Looping"
      ],
      "metadata": {
        "id": "w-TyTY9pwmyF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sum the numbers from 0 to n-1 in different ways\n",
        "\n",
        "* https://www.youtube.com/watch?v=Qgevy75co8c"
      ],
      "metadata": {
        "id": "ttlI9gQjwytH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import timeit"
      ],
      "metadata": {
        "id": "SYO7KwYkwmnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def while_loop(n=100_000_000):\n",
        "  i = 0\n",
        "  s = 0\n",
        "  # Loop in Python\n",
        "  while i < n:\n",
        "    s += 1\n",
        "    i += 1 # Done in Python\n",
        "  return s\n",
        "\n",
        "def for_loop(n=100_000_000):\n",
        "  s = 0\n",
        "  # Iteration, bounds checking done in C\n",
        "  for i in range(n):\n",
        "    s += 1 # Done in Python\n",
        "  return s"
      ],
      "metadata": {
        "id": "0h2247n0wwit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('While Loop:\\t\\t',timeit.timeit(while_loop, number=1))\n",
        "print('  For Loop:\\t\\t',timeit.timeit(for_loop, number=1))\n"
      ],
      "metadata": {
        "id": "cTvSvsC-wmqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def for_loop_with_increment(n=100_000_000):\n",
        "  s = 0\n",
        "  for i in range(n):\n",
        "    s += i\n",
        "    i += i # Redundant\n",
        "  return s\n",
        "\n",
        "def for_loop_with_test(n=100_000_000):\n",
        "  s = 0\n",
        "  for i in range(n):\n",
        "    if i < n: pass\n",
        "    s += i\n",
        "  return s\n",
        "\n",
        "def for_loop_with_increment_and_test(n=100_000_000):\n",
        "  s = 0\n",
        "  for i in range(n):\n",
        "    if i < n: pass\n",
        "    i += 1\n",
        "    s += 100_000_000\n",
        "  return s\n",
        "\n"
      ],
      "metadata": {
        "id": "axQRuwCrwmkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('While Loop:\\t\\t',timeit.timeit(while_loop, number=1))\n",
        "print('  For Loop:\\t\\t',timeit.timeit(for_loop, number=1))\n",
        "\n",
        "print('     For inc Loop:\\t\\t',timeit.timeit(for_loop_with_increment, number=1))\n",
        "print('    For test Loop:\\t\\t',timeit.timeit(for_loop_with_test, number=1))\n",
        "print('For inc+test Loop:\\t\\t',timeit.timeit(for_loop_with_increment_and_test, number=1))\n"
      ],
      "metadata": {
        "id": "iY03KRSKym5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Built-Ins"
      ],
      "metadata": {
        "id": "tDRgqh-gzhww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lazy build of array\n",
        "\n",
        "def sum_range(n=100_000_000):\n",
        "  return sum(range(n))"
      ],
      "metadata": {
        "id": "yWB6ZjUkym2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('  For Loop:\\t\\t',timeit.timeit(sum_range, number=1))"
      ],
      "metadata": {
        "id": "cgCx8T2EwmhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Numpy"
      ],
      "metadata": {
        "id": "6SnogiIqzjtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "6stT58wrzdtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build 100M array in memory\n",
        "\n",
        "def sum_numpy(n=100_000_000):\n",
        "  return np.sum(np.arange(n))"
      ],
      "metadata": {
        "id": "WstsGJr6zdmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('  For Loop:\\t\\t',timeit.timeit(sum_numpy, number=1))"
      ],
      "metadata": {
        "id": "FUsEVo0Pzdi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Formula"
      ],
      "metadata": {
        "id": "Yfp1cExDz4CF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sum_math(n=100_000_000):\n",
        "  return (n * (n-1)) // 2"
      ],
      "metadata": {
        "id": "J8ohc-jnz29I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('  For Loop:\\t\\t',timeit.timeit(sum_math, number=1))"
      ],
      "metadata": {
        "id": "vZTl-Q8rzdf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hHaV-lCb0TkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2htc0I_90Tgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4wzi2bt90TdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LkEK6KpDzddH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Numba\n",
        "\n",
        "* [Make Python 1000x Faster with One Line (6:50)](https://www.youtube.com/watch?v=OiMZtjSZVOw)\n"
      ],
      "metadata": {
        "id": "ORRhMdKau_SO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time, sleep\n",
        "from numba import njit"
      ],
      "metadata": {
        "id": "yXYAex90u-FA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f():\n",
        "  l = []\n",
        "  for x in range(10):\n",
        "    for y in range(1000):\n",
        "      for z in range(10000):\n",
        "        if (x+y+z)/10 == x:\n",
        "          l.append(x)\n",
        "    print(x)\n",
        "  return l\n",
        "\n",
        "start = time()\n",
        "f()\n",
        "print(f\"Finished after {round(time()-start,2)} seconds\")"
      ],
      "metadata": {
        "id": "k-3kgjcwu9z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cannot use many libraries numba is not aware of\n",
        "# First time slower due to compilation\n",
        "\n",
        "@njit\n",
        "def f():\n",
        "  l = []\n",
        "  for x in range(10):\n",
        "    for y in range(1000):\n",
        "      for z in range(10000):\n",
        "        if (x+y+z)/10 == x:\n",
        "          l.append(x)\n",
        "    print(x)\n",
        "  return l\n",
        "\n",
        "start = time()\n",
        "f()\n",
        "print(f\"Finished after {round(time()-start,2)} seconds\")"
      ],
      "metadata": {
        "id": "hclXqHZSvhwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cannot use many libraries numba is not aware of\n",
        "# First time slower due to compilation\n",
        "\n",
        "@njit\n",
        "def f():\n",
        "  l = []\n",
        "  for x in range(10):\n",
        "    for y in range(1000):\n",
        "      for z in range(10000):\n",
        "        if (x+y+z)/10 == x:\n",
        "          l.append(x)\n",
        "    print(x)\n",
        "  return l\n",
        "\n",
        "start = time()\n",
        "for x in range(10000):\n",
        "  f()\n",
        "print(f\"Finished after {round(time()-start,2)} seconds\")"
      ],
      "metadata": {
        "id": "_5JlMMF3wM4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# JAX Crash Course\n",
        "\n",
        "## Outline\n",
        "- What is JAX?\n",
        "- Speed comparison\n",
        "- Replacement for NumPy\n",
        "- Speed up computations with jit()\n",
        "- Automatic Differentiation with grad()\n",
        "- Automatic Vectorization with vmap()\n",
        "- Automatic Parallelization with pmap()\n",
        "- Example Training Loop With JAX\n",
        "- What's the catch?\n",
        "- Summary\n",
        "\n",
        "## Resources:\n",
        "- [JAX Performance Tutorial - YouTube](https://www.youtube.com/watch?v=juo5G3t4qAo\n",
        "- [Official JAX Website](https://jax.readthedocs.io/en/latest/index.html)\n",
        "- [Why You Should (or Shouldn't) be Using Google's JAX in 2022](https://www.assemblyai.com/blog/why-you-should-or-shouldnt-be-using-jax-in-2022/)\n",
        "\n",
        "## Transcribed:\n",
        "\n",
        "- 01 Oct 2022 by Jon Chun"
      ],
      "metadata": {
        "id": "WlevbFFcOmdX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is JAX?\n",
        "JAX is Autograd and [XLA](https://www.tensorflow.org/xla), brought together for high-performance numerical computing and machine learning research. It provides composable transformations of Python+NumPy programs: differentiate, vectorize, parallelize, Just-In-Time compile to GPU/TPU, and more.\n",
        "\n",
        "In simpler words: JAX is NumPy on the CPU, GPU, and TPU, with great automatic differentiation for high-performance machine learning research.\n",
        "\n",
        "#### XLA\n",
        "XLA: Accelerated Linear Algebra, lies at the foundation of what makes JAX so powerful. Developed by Google, XLA is a domain-specific, graph-based, just-in-time compiler for linear algebra.\n",
        "\n",
        "It significantly improves execution speed and lowers memory usage by fusing low-level operations.\n"
      ],
      "metadata": {
        "id": "yQCrc7d_Pjww"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Speed Comparison\n",
        "Huge performance boost when running on GPU/TPU, but even on CPU!"
      ],
      "metadata": {
        "id": "qWD21Ki-PvXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def fn(x):\n",
        "  return x + x*x + x*x*x\n",
        "\n",
        "x = np.random.randn(10000, 10000).astype(dtype='float32')\n",
        "%timeit -n5 fn(x)"
      ],
      "metadata": {
        "id": "KWHJk7sPPyad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "from jax import jit\n",
        "\n",
        "jax_fn = jit(fn)\n",
        "x = jnp.array(x)\n",
        "%timeit -n5 jax_fn(x).block_until_ready()\n",
        "# Use block_until_ready because JAX uses asynchronous execution by default"
      ],
      "metadata": {
        "id": "CnX2ay0TP2AF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Drop-in Replacement for NumPy\n",
        "JAX numpy can often be used as drop-in repcacement for numpy since the API is almost identical. One useful feature of JAX is that the same code can be run on different backends – CPU, GPU and TPU."
      ],
      "metadata": {
        "id": "s2E5gaz9QOSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "x_np = np.linspace(0, 10, 1000)\n",
        "y_np = 2 * np.sin(x_np) * np.cos(x_np)\n",
        "plt.plot(x_np, y_np)"
      ],
      "metadata": {
        "id": "V2qUtvH5QRau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "\n",
        "x_jnp = jnp.linspace(0, 10, 1000)\n",
        "y_jnp = 2 * jnp.sin(x_jnp) * jnp.cos(x_jnp)\n",
        "plt.plot(x_jnp, y_jnp)"
      ],
      "metadata": {
        "id": "O63kYGWCQV1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using this alone can bring a nice performance boost on the GPU/TPU.\n",
        "\n",
        "**Big difference: JAX arrays are immutable!**"
      ],
      "metadata": {
        "id": "olKbYupoQb7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.arange(10)\n",
        "x[0] = 10\n",
        "x"
      ],
      "metadata": {
        "id": "ZmDKEcxdQjd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = jnp.arange(10)\n",
        "x[0] = 10"
      ],
      "metadata": {
        "id": "-4ryabwFQaI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Speed up computations with jit()\n",
        "Just-in-time, or JIT compilation, is a method of executing code that lies between interpretation and ahead-of-time (AoT) compilation.\n",
        "\n",
        "The important fact is that a JIT-compiler will compile code at runtime into a fast executable (at the cost of a slower first run)."
      ],
      "metadata": {
        "id": "2N7HhPIaTBR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fn(x):\n",
        "  return x + x*x + x*x*x\n",
        "\n",
        "x_np = np.random.randn(5000, 5000).astype(dtype='float32')\n",
        "%timeit -n5 fn(x_np)\n",
        "\n",
        "x_jnp = jnp.array(x_np)\n",
        "%timeit -n5 fn(x_jnp).block_until_ready()\n",
        "\n",
        "jitted = jit(fn)\n",
        "%timeit -n5 jitted(x_jnp).block_until_ready()"
      ],
      "metadata": {
        "id": "U2GA4pvyQ1XC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use as decorator\n",
        "@jit\n",
        "def fn_jitted(x):\n",
        "  return x + x*x + x*x*x\n",
        "\n",
        "%timeit -n5 fn_jitted(x_jnp).block_until_ready()"
      ],
      "metadata": {
        "id": "cx4tI51ATiC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How does JIT works?\n",
        "\n",
        "JIT and other JAX transforms work by tracing a function to determine its effect on inputs of a specific shape and type."
      ],
      "metadata": {
        "id": "Y3zI_MYsaV8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@jit\n",
        "def f(x, y):\n",
        "  print(\"Running f():\")\n",
        "  print(f\"  x = {x}\")\n",
        "  print(f\"  y = {y}\")\n",
        "  result = jnp.dot(x + 1, y + 1)\n",
        "  print(f\"  result = {result}\")\n",
        "  return result\n",
        "\n",
        "x = np.random.randn(3, 4)\n",
        "y = np.random.randn(4)"
      ],
      "metadata": {
        "id": "fqjIGPWPT6Wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f(x, y)"
      ],
      "metadata": {
        "id": "s2acvEDiXjlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f(x, y)"
      ],
      "metadata": {
        "id": "C1eojc_XXkYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the print statements execute, but rather than printing the data we passed to the function, though, it prints tracer objects that stand-in for them.\n",
        "\n",
        "These tracer objects are what jax.jit uses to extract the sequence of operations specified by the function. Basic tracers are stand-ins that encode the shape and dtype of the arrays, but are agnostic to the values. This recorded sequence of computations can then be efficiently applied within XLA to new inputs with the same shape and dtype, without having to re-execute the Python code.\n",
        "\n",
        "When we call the compiled function again on matching inputs, no re-compilation is required and nothing is printed because the result is computed in compiled XLA rather than in Python."
      ],
      "metadata": {
        "id": "zMYVvXdxX8F7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Limitation of JIT\n",
        "**Important**: because JIT compilation is done without information on the content of the array, control flow statements in the function cannot depend on traced values."
      ],
      "metadata": {
        "id": "lkm_XC9zaphA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Important: because JIT compilation is done without information\n",
        "# on the content of the array, control flow statements in the function cannot depend\n",
        "# on traced values\n",
        "def f(x):\n",
        "  if x > 0:\n",
        "    return x\n",
        "  else:\n",
        "    return 2 * x\n",
        "\n",
        "f_jit = jit(f)\n",
        "f_jit(10)  # Should raise an error."
      ],
      "metadata": {
        "id": "yL-c9yz6YNwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pure Functions\n",
        "It needs [pure functions](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#pure-functions):\n",
        "\n",
        "- No control flow statements that depend on values\n",
        "- It cannot use or change global state (variables outside its scope, global variables)\n",
        "- It cannot have an I/O stream - so no printing, asking for input, or accessing the time\n",
        "- It cannot have a mutable function as an argument (which a concurrent process could modify)\n",
        "\n",
        "#### Untracked side effects!\n",
        "Silently throw off the results of your calculation you should be sure that you understand how to write pure functions if you plan to utilize JAX. Read more [here](https://jax.readthedocs.io/en/latest/faq.html#jit-changes-the-behavior-of-my-function)."
      ],
      "metadata": {
        "id": "QUm6wTIyY9wU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Automatic Differentiation with grad()\n",
        "\n",
        "Very simple to calculate gradients and second/third order gradients etc.\n",
        "\n",
        "The gradients follow more the underlying math rather than using backpropagation in other Deep Learning libraries --> It can be much faster with JAX!\n",
        "\n",
        "grad() works for scalar-valued function, meaning a function which maps scalars/vectors to scalars."
      ],
      "metadata": {
        "id": "Hj6TOVcca8IR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import grad\n",
        "\n",
        "def f(x):\n",
        "  return x**3 + 2*x**2 - 3*x + 1\n",
        "\n",
        "dfdx = grad(f)\n",
        "\n",
        "d2fdx = grad(grad(f))\n",
        "#d2fdx = grad(dfdx)\n",
        "\n",
        "print(f\"x = 1.0\")\n",
        "print(f\"f  (x) = {f(1.)}\")\n",
        "print(f\"f' (x) = 3*x^2 + 4x - 3 = {dfdx(1.)}\")\n",
        "print(f\"f''(x) = 6x + 4 = {d2fdx(1.)}\")"
      ],
      "metadata": {
        "id": "74XvEc7-YOI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Another example\n",
        "def rectified_cube(x):\n",
        "  r = 1\n",
        "\n",
        "  if x < 0.:\n",
        "    for i in range(3):\n",
        "      r *= x\n",
        "    r = -r\n",
        "  else:\n",
        "    for i in range(3):\n",
        "        r *= x\n",
        "\n",
        "  return r\n",
        "\n",
        "gradient_function = grad(rectified_cube)\n",
        "\n",
        "print(f\"x = 2   f(x) = {rectified_cube(2.)}   f'(x) =  3*x^2 = {gradient_function(2.)}\")\n",
        "print(f\"x = -3  f(x) = {rectified_cube(-3.)}  f'(x) = -3*x^2 = {gradient_function(-3.)}\")"
      ],
      "metadata": {
        "id": "MJC3toxYbAwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use argnum\n",
        "def sum_squared_error(x, y):\n",
        "  return jnp.sum((x-y)**2)\n",
        "\n",
        "sum_squared_error_dx = grad(sum_squared_error)\n",
        "\n",
        "x = jnp.asarray([1.0, 2.0, 3.0, 4.0])\n",
        "y = jnp.asarray([1.1, 2.1, 3.1, 4.1])\n",
        "\n",
        "print(sum_squared_error_dx(x, y))\n",
        "\n",
        "sum_squared_error_dy = grad(sum_squared_error, argnums=(1))\n",
        "\n",
        "print(sum_squared_error_dy(x, y))"
      ],
      "metadata": {
        "id": "TbVw30Ltb-jV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Automatic Parallelization with pmap()\n",
        "Performing distributed training of SPMD programs with a single command\n",
        "\n",
        "Consider the example of vector-matrix multiplication. Let’s say we are performing this computation by sequentially computing the dot product of the vector with each row of the matrix. We would need to push these computations through our hardware one at a time.\n",
        "\n",
        "With JAX, we can easily distribute these computations across 4 TPUs by simply wrapping our operation in pmap(). This allows us to concurrently perform one dot product on each TPU, significantly increasing our computation speed (for large computations)"
      ],
      "metadata": {
        "id": "N6vOISF4ebx3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example Training Loop with JAX\n"
      ],
      "metadata": {
        "id": "W2P0r1WufO_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "xs = np.random.normal(size=(100,))\n",
        "noise = np.random.normal(scale=0.1, size=(100,))\n",
        "ys = xs * 3 - 1 + noise\n",
        "\n",
        "plt.scatter(xs, ys)"
      ],
      "metadata": {
        "id": "ELQld9gkdxZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model(theta, x):\n",
        "  \"\"\"Computes wx + b on a batch of input x.\"\"\"\n",
        "  w, b = theta\n",
        "  return w * x + b\n",
        "\n",
        "def loss_fn(theta, x, y):\n",
        "  prediction = model(theta, x)\n",
        "  return jnp.mean((prediction-y)**2)\n",
        "\n",
        "def update(theta, x, y, lr=0.1):\n",
        "  return theta - lr * grad(loss_fn)(theta, x, y)"
      ],
      "metadata": {
        "id": "pirqblCQfyaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "theta = jnp.array([1., 1.])\n",
        "\n",
        "for _ in range(1000):\n",
        "  theta = update(theta, xs, ys)\n",
        "\n",
        "plt.scatter(xs, ys)\n",
        "plt.plot(xs, model(theta, xs))\n",
        "\n",
        "w, b = theta\n",
        "print(f\"w: {w:<.2f}, b: {b:<.2f}\")\n",
        "print(f\"f(x) = {w:<.2f} * x {b:<.2f}\")"
      ],
      "metadata": {
        "id": "AnciC0cYf0qS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What's the catch\n",
        "\n",
        "- Functional Paradigm: Needs pure functions\n",
        "- Untracked side effects\n",
        "- Inability to modify arrays\n",
        "- Explicit [PRNG handling](https://jax.readthedocs.io/en/latest/jax.random.html#prng-keys). Unlike the stateful pseudorandom number generators (PRNGs) that users of NumPy and SciPy may be accustomed to, JAX random functions all require an explicit PRNG state to be passed as a first argument. --> Be careful when you want to reproduce results."
      ],
      "metadata": {
        "id": "X379C2o9e-ct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "JAX is a very promising project and has been steadily growing in popularity, despite the learning curve introduced by its functional paradigm.\n",
        "\n",
        "- Easy to use as drop-in replacement for numpy\n",
        "- Autograd is extremelty useful for scientific computing\n",
        "- Deep Learning projects can benefit from this!\n",
        "- Huge performance improvement possible!!\n",
        "- But be careful and make sure to understand pure functions and how jit works\n",
        "- JAX is still officially considered an experimental framework."
      ],
      "metadata": {
        "id": "xwheB9Ceh_BO"
      }
    }
  ]
}